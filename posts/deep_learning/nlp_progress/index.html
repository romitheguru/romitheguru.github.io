<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>From LSTMs to RLHF — How One Idea Ignites the Next | Romee's Blog</title>
<meta name=keywords content="deep learning,transformers,NLP"><meta name=description content="Innovation as a Relay Race
Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.
Tracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting."><meta name=author content="Romee Panchal"><link rel=canonical href=https://romitheguru.github.io/posts/deep_learning/nlp_progress/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://romitheguru.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://romitheguru.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://romitheguru.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://romitheguru.github.io/apple-touch-icon.png><link rel=mask-icon href=https://romitheguru.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://romitheguru.github.io/posts/deep_learning/nlp_progress/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-86W3800S9B"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-86W3800S9B")}</script><meta property="og:url" content="https://romitheguru.github.io/posts/deep_learning/nlp_progress/"><meta property="og:site_name" content="Romee's Blog"><meta property="og:title" content="From LSTMs to RLHF — How One Idea Ignites the Next"><meta property="og:description" content="Innovation as a Relay Race Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.
Tracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-12T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Transformers"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="From LSTMs to RLHF — How One Idea Ignites the Next"><meta name=twitter:description content="Innovation as a Relay Race
Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.
Tracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://romitheguru.github.io/posts/"},{"@type":"ListItem","position":2,"name":"From LSTMs to RLHF — How One Idea Ignites the Next","item":"https://romitheguru.github.io/posts/deep_learning/nlp_progress/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"From LSTMs to RLHF — How One Idea Ignites the Next","name":"From LSTMs to RLHF — How One Idea Ignites the Next","description":"Innovation as a Relay Race Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.\nTracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting.\n","keywords":["deep learning","transformers","NLP"],"articleBody":"Innovation as a Relay Race Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.\nTracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting.\nBelow is a narrative — rather than a checklist — of the papers that turned yesterday’s impossibilities into today’s defaults.\nGlobal and Contextual Word Representation GloVe (2014) argued that local window-based prediction (word2vec) ignores global statistics, so it factorises a log-co-occurrence matrix instead. The jump in analogy accuracy hinted that richer context matters.\nFour years later, ELMo (2018) showed an even bigger flaw: static embeddings cannot capture homonyms or polysemy. By feeding sentences through a bi-directional LSTM language model and taking layer-weighted mixtures, ELMo made word meaning conditional on the whole sequence — fuel for the first double-digit gains across QA and coreference tasks.\nThat insight reached escape velocity with BERT (2018). Replace RNNs with Transformers, mask 15 % of tokens, predict them using bidirectional context, and you have a universal encoder that can be fine-tuned for almost anything. RoBERTa (2019) then asked a heretical question: What if BERT’s magic is just large batches, more data, and longer training? By stripping away Next-Sentence Prediction and scaling aggressively, it proved that optimisation tricks sometimes trump architecture tweaks.\nRemembering More Than a Page As soon as Transformers won short-sequence tasks, researchers hit the quadratic wall: self-attention’s cost explodes with length. Transformer-XL (2019) revived recurrence — storing hidden states from the previous segment and using a relative positional scheme so that textual horizons could extend from 512 tokens to thousands. The leap mattered for story generation and long-form language modelling.\nWhile Transformer-XL kept the full attention pattern, others chased sparsity. Linformer (2020) projected keys and values to a low-rank space, shrinking complexity from O(n²) to O(nk); Longformer (2020) combined sliding-window locality with a few global tokens to process 8 k-token scientific papers on a single GPU. By 2022, Tay et al.’s survey catalogued more than 50 such variants, signalling consensus that hardware — not theory — is now the bottleneck.\nUnifying Tasks and Upsizing Everything If context length was one ceiling, task-specific heads were another. T5 (2020) demolished both by casting all problems — translation, summarisation, classification — as “input text -\u003e output text.” With a cleaned 750 GB corpus and a seq-to-seq Transformer, it showed that careful task formatting can unlock as much transfer as bigger models.\nSpeaking of bigger: GPT-3 (2020) asked what happens when you simply scale an autoregressive Transformer to 175 B parameters and skip fine-tuning. The answer was in-context learning — the ability to perform new tasks from a few demonstration prompts. Kaplan et al. (2020) then put numbers on the intuition, deriving power-law scaling rules that still guide GPU-budget spreadsheets at every lab.\nAligning Models with Humans Raw capability is useless — or dangerous — if a model ignores human intent. Ouyang et al. (2022) introduced the now-canonical RLHF pipeline: supervised instruction tuning, a reward model trained on preference comparisons, and PPO reinforcement learning to steer generation. That recipe underpins ChatGPT and every serious deployment concerned with safety, helpfulness, or brand tone.\nEmerging Frontiers With text well-tamed, attention is turning to the edges: multimodality (CLIP, 2021), 100 k-token memory via state-space models (S4, 2021), retrieval-augmented generation for factual reliability (RETRO, 2022), and sparse mixtures-of-experts to grow capacity without quadratic FLOPs (Switch Transformer, 2021). Each frontier picks up where the last breakthrough shows strain — proof that the relay race continues.\nThen there is a whole new race of reasoning models.\nReasoning-first models — OpenAI o1 / o3. Instead of squeezing ever more tokens into a single forward pass, these variants slow the clock and explicitly allocate extra computation to step-by-step reasoning, tool use and self-reflection. The result is higher accuracy on multi-hop problems and code synthesis without a parameter-count arms race. DataCampTechTarget Next-gen Mixture-of-Experts — DeepSeek-R1 (671 B total / 37 B active). DeepSeek shows MoE is no longer an exotic Google-only trick: by activating a tiny slice of the network per token, it achieves GPT-4-class math-and-coding scores while running on commodity H800s — an order-of-magnitude cut in training FLOPs. HeidloffModularThe Wall Street Journal Open-source scale — LLaMA 4 (April 2025). Meta’s fourth-generation family adds two open-weights variants (Maverick \u0026 Scout) that match GPT-4-level benchmarks while remaining fine-tune-friendly. For academics and start-ups, it resets the “free-to-tinker” baseline. AiWireWikipedia Ultra-long context — GPT-4.1 (up to 1 M tokens). The new flagship pushes context length from 128 k to a million tokens, enough to hold eight copies of the entire React codebase or a week of Slack history. It also closes the coding-accuracy gap with domain-specific copilots. OpenAITechTargetReuters Vision-enhanced alignment — Claude 3.5 Sonnet. An thropic’s mid-sized model leap-frogs its own Claude 3 Opus on vision and reasoning tasks, accurately reading charts, receipts and low-quality images — critical for retail and logistics workflows. AnthropicDataCamp Each effort tackles the pressure points revealed by the previous wave. These additions reinforce the same relay pattern we’ve seen:\nIdentify a practical ceiling (reasoning depth, training cost, openness, context window, multimodal grounding). Engineer a targeted workaround (scheduled reflection, sparse experts, permissive licensing, memory-efficient attention, vision encoders). Watch the workaround create its follow-up questions (how to debug hidden experts? how to search a million-token prompt?). Conclusion — What This Journey Teaches Progress is iterative but not incremental. A single clever hack (masking, recurrence, RLHF) can open a new design space.\nBottlenecks create research themes. Once representation quality plateaued, context length and compute cost became the urgent problems; now, alignment and grounding dominate.\nAdopt selectively. Map your pain point — representation, context, scale, or alignment — to the paper that solved it. Implement that solution before chasing the next hype wave.\nReferences Hochreiter, S., \u0026 Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780.\nPennington, J., Socher, R., \u0026 Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. EMNLP.\nPeters, M. E., et al. (2018). Deep Contextualized Word Representations. NAACL.\nDevlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL.\nLiu, Y., et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692.\nDai, Z., et al. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL.\nWang, S., et al. (2020). Linformer: Self-Attention with Linear Complexity. arXiv:2006.04768.\nBeltagy, I., Peters, M., \u0026 Cohan, A. (2020). Longformer: The Long-Document Transformer. arXiv:2004.05150.\nRaffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. JMLR, 21(140).\nBrown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS.\nKaplan, J., et al. (2020). Scaling Laws for Neural Language Models. arXiv:2001.08361.\nOuyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155.\nRadford, A., et al. (2021). Learning Transferable Visual Models from Natural Language Supervision. ICML.\nGu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv:2111.00396.\nFedus, W., et al. (2021). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. JMLR, 23(120).\nBorgeaud, S., et al. (2022). Improving Language Models by Retrieving from Trillions of Tokens. Nature, 601, 590–595.\nTay, Y., et al. (2022). Efficient Transformers: A Survey. ACM Computing Surveys, 55(6), 1–28.\nHeidloff, N. (2025). Key Concepts of DeepSeek-R1. Heidloff\nModular AI. (2025). Exploring DeepSeek-R1’s Mixture-of-Experts Architecture. Modular\nMeta AI. (2025). LLaMA 4 release blog \u0026 documentation. AiWire\nOpenAI. (2025). Introducing GPT-4.1 in the API. OpenAI\nReuters. (2025, Apr 14). OpenAI launches GPT-4.1 models with improved coding \u0026 long context. Reuters\nAnthropic. (2024). Introducing Claude 3.5 Sonnet. Anthropic\n","wordCount":"1304","inLanguage":"en","datePublished":"2025-07-12T00:00:00Z","dateModified":"2025-07-12T00:00:00Z","author":{"@type":"Person","name":"Romee Panchal"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://romitheguru.github.io/posts/deep_learning/nlp_progress/"},"publisher":{"@type":"Organization","name":"Romee's Blog","logo":{"@type":"ImageObject","url":"https://romitheguru.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://romitheguru.github.io/ accesskey=h title="Romee's Blog (Alt + H)"><img src=https://romitheguru.github.io/logo_hu_64c0416bc445921c.png alt aria-label=logo height=35>Romee's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://romitheguru.github.io/ title=Home><span>Home</span></a></li><li><a href=https://romitheguru.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://romitheguru.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://romitheguru.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://romitheguru.github.io/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">From LSTMs to RLHF — How One Idea Ignites the Next</h1><div class=post-meta><span title='2025-07-12 00:00:00 +0000 UTC'>July 12, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Romee Panchal</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#innovation-as-a-relay-race aria-label="Innovation as a Relay Race">Innovation as a Relay Race</a></li><li><a href=#global-and-contextual-word-representation aria-label="Global and Contextual Word Representation">Global and Contextual Word Representation</a></li><li><a href=#remembering-more-than-a-page aria-label="Remembering More Than a Page">Remembering More Than a Page</a></li><li><a href=#unifying-tasks-and-upsizing-everything aria-label="Unifying Tasks and Upsizing Everything">Unifying Tasks and Upsizing Everything</a></li><li><a href=#aligning-models-with-humans aria-label="Aligning Models with Humans">Aligning Models with Humans</a></li><li><a href=#emerging-frontiers aria-label="Emerging Frontiers">Emerging Frontiers</a></li><li><a href=#conclusion--what-this-journey-teaches aria-label="Conclusion — What This Journey Teaches">Conclusion — What This Journey Teaches</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=innovation-as-a-relay-race>Innovation as a Relay Race<a hidden class=anchor aria-hidden=true href=#innovation-as-a-relay-race>#</a></h2><p>Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.</p><p>Tracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting.</p><p>Below is a narrative — rather than a checklist — of the papers that turned yesterday’s impossibilities into today’s defaults.</p><h2 id=global-and-contextual-word-representation>Global and Contextual Word Representation<a hidden class=anchor aria-hidden=true href=#global-and-contextual-word-representation>#</a></h2><p>GloVe (2014) argued that local window-based prediction (word2vec) ignores global statistics, so it factorises a log-co-occurrence matrix instead. The jump in analogy accuracy hinted that richer context matters.</p><p>Four years later, ELMo (2018) showed an even bigger flaw: static embeddings cannot capture homonyms or polysemy. By feeding sentences through a bi-directional LSTM language model and taking layer-weighted mixtures, ELMo made word meaning conditional on the whole sequence — fuel for the first double-digit gains across QA and coreference tasks.</p><p>That insight reached escape velocity with BERT (2018). Replace RNNs with Transformers, mask 15 % of tokens, predict them using bidirectional context, and you have a universal encoder that can be fine-tuned for almost anything. RoBERTa (2019) then asked a heretical question: What if BERT’s magic is just large batches, more data, and longer training? By stripping away Next-Sentence Prediction and scaling aggressively, it proved that optimisation tricks sometimes trump architecture tweaks.</p><h2 id=remembering-more-than-a-page>Remembering More Than a Page<a hidden class=anchor aria-hidden=true href=#remembering-more-than-a-page>#</a></h2><p>As soon as Transformers won short-sequence tasks, researchers hit the quadratic wall: self-attention’s cost explodes with length. Transformer-XL (2019) revived recurrence — storing hidden states from the previous segment and using a relative positional scheme so that textual horizons could extend from 512 tokens to thousands. The leap mattered for story generation and long-form language modelling.</p><p>While Transformer-XL kept the full attention pattern, others chased sparsity. Linformer (2020) projected keys and values to a low-rank space, shrinking complexity from O(n²) to O(nk); Longformer (2020) combined sliding-window locality with a few global tokens to process 8 k-token scientific papers on a single GPU. By 2022, Tay et al.’s survey catalogued more than 50 such variants, signalling consensus that hardware — not theory — is now the bottleneck.</p><h2 id=unifying-tasks-and-upsizing-everything>Unifying Tasks and Upsizing Everything<a hidden class=anchor aria-hidden=true href=#unifying-tasks-and-upsizing-everything>#</a></h2><p>If context length was one ceiling, task-specific heads were another. T5 (2020) demolished both by casting all problems — translation, summarisation, classification — as “input text -> output text.” With a cleaned 750 GB corpus and a seq-to-seq Transformer, it showed that careful task formatting can unlock as much transfer as bigger models.</p><p>Speaking of bigger: GPT-3 (2020) asked what happens when you simply scale an autoregressive Transformer to 175 B parameters and skip fine-tuning. The answer was in-context learning — the ability to perform new tasks from a few demonstration prompts. Kaplan et al. (2020) then put numbers on the intuition, deriving power-law scaling rules that still guide GPU-budget spreadsheets at every lab.</p><h2 id=aligning-models-with-humans>Aligning Models with Humans<a hidden class=anchor aria-hidden=true href=#aligning-models-with-humans>#</a></h2><p>Raw capability is useless — or dangerous — if a model ignores human intent. Ouyang et al. (2022) introduced the now-canonical RLHF pipeline: supervised instruction tuning, a reward model trained on preference comparisons, and PPO reinforcement learning to steer generation. That recipe underpins ChatGPT and every serious deployment concerned with safety, helpfulness, or brand tone.</p><h2 id=emerging-frontiers>Emerging Frontiers<a hidden class=anchor aria-hidden=true href=#emerging-frontiers>#</a></h2><p>With text well-tamed, attention is turning to the edges: multimodality (CLIP, 2021), 100 k-token memory via state-space models (S4, 2021), retrieval-augmented generation for factual reliability (RETRO, 2022), and sparse mixtures-of-experts to grow capacity without quadratic FLOPs (Switch Transformer, 2021). Each frontier picks up where the last breakthrough shows strain — proof that the relay race continues.</p><p>Then there is a whole new race of reasoning models.</p><ul><li>Reasoning-first models — OpenAI o1 / o3. Instead of squeezing ever more tokens into a single forward pass, these variants slow the clock and explicitly allocate extra computation to step-by-step reasoning, tool use and self-reflection. The result is higher accuracy on multi-hop problems and code synthesis without a parameter-count arms race. DataCampTechTarget</li><li>Next-gen Mixture-of-Experts — DeepSeek-R1 (671 B total / 37 B active). DeepSeek shows MoE is no longer an exotic Google-only trick: by activating a tiny slice of the network per token, it achieves GPT-4-class math-and-coding scores while running on commodity H800s — an order-of-magnitude cut in training FLOPs. HeidloffModularThe Wall Street Journal</li><li>Open-source scale — LLaMA 4 (April 2025). Meta’s fourth-generation family adds two open-weights variants (Maverick & Scout) that match GPT-4-level benchmarks while remaining fine-tune-friendly. For academics and start-ups, it resets the “free-to-tinker” baseline. AiWireWikipedia</li><li>Ultra-long context — GPT-4.1 (up to 1 M tokens). The new flagship pushes context length from 128 k to a million tokens, enough to hold eight copies of the entire React codebase or a week of Slack history. It also closes the coding-accuracy gap with domain-specific copilots. OpenAITechTargetReuters</li><li>Vision-enhanced alignment — Claude 3.5 Sonnet. An thropic’s mid-sized model leap-frogs its own Claude 3 Opus on vision and reasoning tasks, accurately reading charts, receipts and low-quality images — critical for retail and logistics workflows. AnthropicDataCamp</li></ul><p>Each effort tackles the pressure points revealed by the previous wave. These additions reinforce the same relay pattern we’ve seen:</p><ul><li>Identify a practical ceiling (reasoning depth, training cost, openness, context window, multimodal grounding).</li><li>Engineer a targeted workaround (scheduled reflection, sparse experts, permissive licensing, memory-efficient attention, vision encoders).</li><li>Watch the workaround create its follow-up questions (how to debug hidden experts? how to search a million-token prompt?).</li></ul><h2 id=conclusion--what-this-journey-teaches>Conclusion — What This Journey Teaches<a hidden class=anchor aria-hidden=true href=#conclusion--what-this-journey-teaches>#</a></h2><ul><li><p>Progress is iterative but not incremental. A single clever hack (masking, recurrence, RLHF) can open a new design space.</p></li><li><p>Bottlenecks create research themes. Once representation quality plateaued, context length and compute cost became the urgent problems; now, alignment and grounding dominate.</p></li><li><p>Adopt selectively. Map your pain point — representation, context, scale, or alignment — to the paper that solved it. Implement that solution before chasing the next hype wave.</p></li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><p>Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780.</p></li><li><p>Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. EMNLP.</p></li><li><p>Peters, M. E., et al. (2018). Deep Contextualized Word Representations. NAACL.</p></li><li><p>Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL.</p></li><li><p>Liu, Y., et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692.</p></li><li><p>Dai, Z., et al. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL.</p></li><li><p>Wang, S., et al. (2020). Linformer: Self-Attention with Linear Complexity. arXiv:2006.04768.</p></li><li><p>Beltagy, I., Peters, M., & Cohan, A. (2020). Longformer: The Long-Document Transformer. arXiv:2004.05150.</p></li><li><p>Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. JMLR, 21(140).</p></li><li><p>Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS.</p></li><li><p>Kaplan, J., et al. (2020). Scaling Laws for Neural Language Models. arXiv:2001.08361.</p></li><li><p>Ouyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155.</p></li><li><p>Radford, A., et al. (2021). Learning Transferable Visual Models from Natural Language Supervision. ICML.</p></li><li><p>Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv:2111.00396.</p></li><li><p>Fedus, W., et al. (2021). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. JMLR, 23(120).</p></li><li><p>Borgeaud, S., et al. (2022). Improving Language Models by Retrieving from Trillions of Tokens. Nature, 601, 590–595.</p></li><li><p>Tay, Y., et al. (2022). Efficient Transformers: A Survey. ACM Computing Surveys, 55(6), 1–28.</p></li><li><p>Heidloff, N. (2025). Key Concepts of DeepSeek-R1. Heidloff</p></li><li><p>Modular AI. (2025). Exploring DeepSeek-R1’s Mixture-of-Experts Architecture. Modular</p></li><li><p>Meta AI. (2025). LLaMA 4 release blog & documentation. AiWire</p></li><li><p>OpenAI. (2025). Introducing GPT-4.1 in the API. OpenAI</p></li><li><p>Reuters. (2025, Apr 14). OpenAI launches GPT-4.1 models with improved coding & long context. Reuters</p></li><li><p>Anthropic. (2024). Introducing Claude 3.5 Sonnet. Anthropic</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://romitheguru.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://romitheguru.github.io/tags/transformers/>Transformers</a></li><li><a href=https://romitheguru.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=next href=https://romitheguru.github.io/posts/deep_learning/learning_path/><span class=title>Next »</span><br><span>From Logistic Regression to Transformers: Learning Path</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share From LSTMs to RLHF — How One Idea Ignites the Next on x" href="https://x.com/intent/tweet/?text=From%20LSTMs%20to%20RLHF%20%e2%80%94%20How%20One%20Idea%20Ignites%20the%20Next&amp;url=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2fnlp_progress%2f&amp;hashtags=deeplearning%2ctransformers%2cNLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From LSTMs to RLHF — How One Idea Ignites the Next on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2fnlp_progress%2f&amp;title=From%20LSTMs%20to%20RLHF%20%e2%80%94%20How%20One%20Idea%20Ignites%20the%20Next&amp;summary=From%20LSTMs%20to%20RLHF%20%e2%80%94%20How%20One%20Idea%20Ignites%20the%20Next&amp;source=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2fnlp_progress%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From LSTMs to RLHF — How One Idea Ignites the Next on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2fnlp_progress%2f&title=From%20LSTMs%20to%20RLHF%20%e2%80%94%20How%20One%20Idea%20Ignites%20the%20Next"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From LSTMs to RLHF — How One Idea Ignites the Next on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2fnlp_progress%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From LSTMs to RLHF — How One Idea Ignites the Next on whatsapp" href="https://api.whatsapp.com/send?text=From%20LSTMs%20to%20RLHF%20%e2%80%94%20How%20One%20Idea%20Ignites%20the%20Next%20-%20https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2fnlp_progress%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://romitheguru.github.io/>Romee's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>