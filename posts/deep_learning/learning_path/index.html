<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>From Logistic Regression to Transformers: Learning Path | Romee's Blog</title>
<meta name=keywords content="deep learning,transformers,learning-path"><meta name=description content="Are you fascinated by deep learning&rsquo;s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.
In this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you&rsquo;re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding."><meta name=author content="Romee Panchal"><link rel=canonical href=https://romitheguru.github.io/posts/deep_learning/learning_path/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://romitheguru.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://romitheguru.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://romitheguru.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://romitheguru.github.io/apple-touch-icon.png><link rel=mask-icon href=https://romitheguru.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://romitheguru.github.io/posts/deep_learning/learning_path/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-86W3800S9B"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-86W3800S9B")}</script><meta property="og:url" content="https://romitheguru.github.io/posts/deep_learning/learning_path/"><meta property="og:site_name" content="Romee's Blog"><meta property="og:title" content="From Logistic Regression to Transformers: Learning Path"><meta property="og:description" content="Are you fascinated by deep learning’s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.
In this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you’re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-24T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-24T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Transformers"><meta property="article:tag" content="Learning-Path"><meta property="og:image" content="https://romitheguru.github.io/transformers.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://romitheguru.github.io/transformers.png"><meta name=twitter:title content="From Logistic Regression to Transformers: Learning Path"><meta name=twitter:description content="Are you fascinated by deep learning&rsquo;s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.
In this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you&rsquo;re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://romitheguru.github.io/posts/"},{"@type":"ListItem","position":2,"name":"From Logistic Regression to Transformers: Learning Path","item":"https://romitheguru.github.io/posts/deep_learning/learning_path/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"From Logistic Regression to Transformers: Learning Path","name":"From Logistic Regression to Transformers: Learning Path","description":"Are you fascinated by deep learning\u0026rsquo;s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.\nIn this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you\u0026rsquo;re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding.\n","keywords":["deep learning","transformers","learning-path"],"articleBody":"Are you fascinated by deep learning’s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.\nIn this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you’re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding.\nHere’s a detailed week-by-week learning path. Each week we will build upon previous knowledge:\nWeek 1: Linear Models Topics:\nLogistic regression (binary classification) Cross-entropy loss Softmax function for multi-class problems Deep dive into gradient descent variants (SGD, Mini-batch) Resources:\nhttps://www.youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI 3Blue1Brown Neural Network video series Article: A Visual Explanation of Softmax Regression Project:\nImplement logistic regression from scratch using NumPy. Use sklearn for logistic and softmax regression on sample datasets. Week 2: Neural Network Foundations Topics:\nSingle-layer and multi-layer perceptrons Activation functions (ReLU, tanh) Forward and backward propagation Derivation of backpropagation Resources\nDeep Learning\" by Ian Goodfellow – Chapter 6 (Feedforward Neural Networks) Stanford CS231n lecture notes on backprop TensorFlow Playground to visualize FFNNs https://www.sscardapane.it/alice-book/ Project\nImplement a basic FFNN from scratch (with one hidden layer). Create a simple feedforward neural network (FFNN) to classify the MNIST digits dataset using a framework like PyTorch or TensorFlow. Experiment with different activation functions (ReLU, sigmoid) and compare performance. Week 3: Deep Neural Networks Topics:\nMultiple hidden layers Advanced activation functions Initialization techniques Basic optimization algorithms (Momentum, RMSprop) Resources:\nAdam optimizer paper FastAI Deep Learning Course Part 1 PyTorch tutorials Neural Networks and Deep Learning by Michael Nielsen Project:\nImage classification on CIFAR-10 with deep neural network Apply gradient descent with different learning rates and optimizers (SGD, Adam). Week 4: Advanced Optimization \u0026 Regularization Topics:\nBatch normalization Dropout L1/L2 regularization Learning rate scheduling Resources:\nBatch Normalization paper Dropout paper Stanford CS224n lectures Project:\nBuild a deep network for sentiment analysis with regularization techniques Week 5: Sequential Data \u0026 RNNs Topics:\nRNN architecture Backpropagation through time Vanishing/exploding gradients LSTM cells Resources:\nLSTM paper Andrej Karpathy’s blog on RNNs Chris Olah’s LSTM blog post Project:\nCharacter-level text generation using LSTM Week 6: Introduction to Attention Mechanisms Topics:\nEncoder-decoder architecture Teacher forcing Beam search Basic attention mechanisms Resources:\nSeq2Seq paper Neural Machine Translation paper Lilian Weng’s attention blog post Project:\nImplement Bahdanau (additive) or Luong (multiplicative) attention. Implement a basic sequence-to-sequence model for translating English to French using Bahdanau attention (use small parallel corpora). Week 7: Self-Attention and Multi-Head Attention Topics:\nScore functions Query-Key-Value concept Self-attention Dot-product attention vs. additive attention Resources:\nAttention Is All You Need Jay Alammar’s blog on attention https://jalammar.github.io/illustrated-transformer/ Project:\nManually compute self-attention for a toy example and build a self-attention layer using PyTorch. Extend the implementation to a multi-head attention mechanism and validate its performance on sequence data. Week 8: Multi-Head Attention and Positional Encoding Topics:\nMulti-head attention Positional encodings (sinusoidal functions) Resources:\nAttention Is All You Need https://jalammar.github.io/illustrated-transformer/ https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853 https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a Project:\nWrite code for multi-head attention. Implement positional encoding and visualize it. Implement a custom multi-head attention module and add positional encodings. Use this to classify sequences of text (e.g., positive/negative sentiment). Week 9: Transformers Block (Encoder-Decoder Structure) Topics:\nEncoder and decoder architecture Residual connections and layer normalization Resources:\nhttps://nlp.seas.harvard.edu/annotated-transformer/ https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face Projects:\nBuild a simple transformer encoder layer. Build a transformer encoder for a language modeling task using PyTorch or TensorFlow. Train the encoder on a small text dataset (e.g., Shakespeare sonnets). Week 10: Full Transformer Model Topics:\nEnd-to-end implementation of the original transformer Complete transformer architecture Resources:\nhttps://arxiv.org/abs/1706.03762 https://nlp.seas.harvard.edu/annotated-transformer/ https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face Projects:\nImplement a transformer-based sequence classification task. Implement a simplified transformer model from scratch and apply it to text summarization or machine translation. Use performance metrics (BLEU score for translation, ROUGE score for summarization) to evaluate results. Week 11: Transformer Variants (BERT, GPT) Topics:\nBERT (masked language modeling) GPT (causal language modeling) Resources:\nhttps://arxiv.org/abs/1810.04805 https://arxiv.org/abs/2005.14165 https://huggingface.co/learn/nlp-course/en/chapter4/2 Projects:\nFine-tune a pre-trained BERT or GPT model using HuggingFace. Implement a chatbot using a GPT model for conversational responses. Additional Project Ideas Once you complete the core projects, reinforce your learning with larger, integrative projects:\nSentiment Analysis on Movie Reviews: Use transformers for sentiment classification on the IMDB dataset. Named Entity Recognition (NER): Implement NER using transformers and fine-tune on the CoNLL-2003 dataset. Question Answering System: Use BERT or RoBERTa to create a question-answering application on a custom dataset. Week 12: Advanced Techniques and Optimization Topics:\nModel distillation Reducing memory consumption (efficient transformers) Resources:\nhttps://arxiv.org/abs/2009.06732 https://arxiv.org/abs/2001.04451 https://huggingface.co/docs/transformers/en/training Projects:\nExperiment with efficient transformer architectures (e.g., Reformer or Longformer) for a custom dataset with long sequences. Apply model distillation to compress a large transformer model into a smaller, faster one for inference. Happy learning!\nGet the latest articles on AI delivered straight to your inbox. Subscribe here!\n","wordCount":"827","inLanguage":"en","image":"https://romitheguru.github.io/transformers.png","datePublished":"2025-01-24T00:00:00Z","dateModified":"2025-01-24T00:00:00Z","author":{"@type":"Person","name":"Romee Panchal"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://romitheguru.github.io/posts/deep_learning/learning_path/"},"publisher":{"@type":"Organization","name":"Romee's Blog","logo":{"@type":"ImageObject","url":"https://romitheguru.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://romitheguru.github.io/ accesskey=h title="Romee's Blog (Alt + H)"><img src=https://romitheguru.github.io/logo_hu_64c0416bc445921c.png alt aria-label=logo height=35>Romee's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://romitheguru.github.io/ title=Home><span>Home</span></a></li><li><a href=https://romitheguru.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://romitheguru.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://romitheguru.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://romitheguru.github.io/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">From Logistic Regression to Transformers: Learning Path</h1><div class=post-meta><span title='2025-01-24 00:00:00 +0000 UTC'>January 24, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Romee Panchal</div></header><figure class=entry-cover><a href=https://romitheguru.github.io/posts/deep_learning/learning_path/transformers.png target=_blank rel="noopener noreferrer"><img loading=eager srcset='https://romitheguru.github.io/posts/deep_learning/learning_path/transformers_hu_b28e8b13aba75988.png 360w,https://romitheguru.github.io/posts/deep_learning/learning_path/transformers_hu_2f465e80e3319452.png 480w,https://romitheguru.github.io/posts/deep_learning/learning_path/transformers_hu_6d7e5b6d31952416.png 720w,https://romitheguru.github.io/posts/deep_learning/learning_path/transformers.png 727w' src=https://romitheguru.github.io/posts/deep_learning/learning_path/transformers.png sizes="(min-width: 768px) 720px, 100vw" width=727 height=1024 alt="Transformers Architecture"></a><figcaption>The encoder-decoder structure of the Transformer architecture from “Attention Is All You Need“</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#week-1-linear-models aria-label="Week 1: Linear Models">Week 1: Linear Models</a></li><li><a href=#week-2-neural-network-foundations aria-label="Week 2: Neural Network Foundations">Week 2: Neural Network Foundations</a></li><li><a href=#week-3-deep-neural-networks aria-label="Week 3: Deep Neural Networks">Week 3: Deep Neural Networks</a></li><li><a href=#week-4-advanced-optimization--regularization aria-label="Week 4: Advanced Optimization & Regularization">Week 4: Advanced Optimization & Regularization</a></li><li><a href=#week-5-sequential-data--rnns aria-label="Week 5: Sequential Data & RNNs">Week 5: Sequential Data & RNNs</a></li><li><a href=#week-6-introduction-to-attention-mechanisms aria-label="Week 6: Introduction to Attention Mechanisms">Week 6: Introduction to Attention Mechanisms</a></li><li><a href=#week-7-self-attention-and-multi-head-attention aria-label="Week 7: Self-Attention and Multi-Head Attention">Week 7: Self-Attention and Multi-Head Attention</a></li><li><a href=#week-8-multi-head-attention-and-positional-encoding aria-label="Week 8: Multi-Head Attention and Positional Encoding">Week 8: Multi-Head Attention and Positional Encoding</a></li><li><a href=#week-9-transformers-block-encoder-decoder-structure aria-label="Week 9: Transformers Block (Encoder-Decoder Structure)">Week 9: Transformers Block (Encoder-Decoder Structure)</a></li><li><a href=#week-10-full-transformer-model aria-label="Week 10: Full Transformer Model">Week 10: Full Transformer Model</a></li><li><a href=#week-11-transformer-variants-bert-gpt aria-label="Week 11: Transformer Variants (BERT, GPT)">Week 11: Transformer Variants (BERT, GPT)</a><ul><li><a href=#additional-project-ideas aria-label="Additional Project Ideas">Additional Project Ideas</a></li></ul></li><li><a href=#week-12-advanced-techniques-and-optimization aria-label="Week 12: Advanced Techniques and Optimization">Week 12: Advanced Techniques and Optimization</a></li></ul></div></details></div><div class=post-content><p>Are you fascinated by deep learning&rsquo;s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.</p><p>In this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you&rsquo;re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding.</p><p>Here&rsquo;s a detailed week-by-week learning path. Each week we will build upon previous knowledge:</p><h2 id=week-1-linear-models>Week 1: Linear Models<a hidden class=anchor aria-hidden=true href=#week-1-linear-models>#</a></h2><p><strong>Topics:</strong></p><ul><li>Logistic regression (binary classification)</li><li>Cross-entropy loss</li><li>Softmax function for multi-class problems</li><li>Deep dive into gradient descent variants (SGD, Mini-batch)</li></ul><p><strong>Resources:</strong></p><ul><li><a href="https://www.youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI">https://www.youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI</a></li><li><a href=https://www.3blue1brown.com/lessons/neural-networks>3Blue1Brown Neural Network video series</a></li><li><a href=https://d2l.ai/chapter_linear-classification/softmax-regression.html>Article: A Visual Explanation of Softmax Regression</a></li></ul><p><strong>Project:</strong></p><ul><li>Implement logistic regression from scratch using NumPy.</li><li>Use <code>sklearn</code> for logistic and softmax regression on sample datasets.</li></ul><h2 id=week-2-neural-network-foundations>Week 2: Neural Network Foundations<a hidden class=anchor aria-hidden=true href=#week-2-neural-network-foundations>#</a></h2><p><strong>Topics:</strong></p><ul><li>Single-layer and multi-layer perceptrons</li><li>Activation functions (ReLU, tanh)</li><li>Forward and backward propagation</li><li>Derivation of backpropagation</li></ul><p>Resources</p><ul><li><em>Deep Learning"</em> by Ian Goodfellow – Chapter 6 (Feedforward Neural Networks)</li><li><a href=https://cs231n.github.io/optimization-2/>Stanford CS231n lecture notes on backprop</a></li><li>TensorFlow Playground to visualize FFNNs</li><li><a href=https://www.sscardapane.it/alice-book/>https://www.sscardapane.it/alice-book/</a></li></ul><p>Project</p><ul><li>Implement a basic FFNN from scratch (with one hidden layer).</li><li>Create a simple feedforward neural network (FFNN) to classify the MNIST digits dataset using a framework like PyTorch or TensorFlow.</li><li>Experiment with different activation functions (ReLU, sigmoid) and compare performance.</li></ul><h2 id=week-3-deep-neural-networks>Week 3: Deep Neural Networks<a hidden class=anchor aria-hidden=true href=#week-3-deep-neural-networks>#</a></h2><p><strong>Topics:</strong></p><ul><li>Multiple hidden layers</li><li>Advanced activation functions</li><li>Initialization techniques</li><li>Basic optimization algorithms (Momentum, RMSprop)</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1412.6980>Adam optimizer paper</a></li><li>FastAI Deep Learning Course Part 1</li><li><a href=https://pytorch.org/tutorials/>PyTorch tutorials</a></li><li><a href=http://neuralnetworksanddeeplearning.com><em>Neural Networks and Deep Learning</em></a> by Michael Nielsen</li></ul><p><strong>Project:</strong></p><ul><li>Image classification on CIFAR-10 with deep neural network</li><li>Apply gradient descent with different learning rates and optimizers (SGD, Adam).</li></ul><h2 id=week-4-advanced-optimization--regularization>Week 4: Advanced Optimization & Regularization<a hidden class=anchor aria-hidden=true href=#week-4-advanced-optimization--regularization>#</a></h2><p><strong>Topics:</strong></p><ul><li>Batch normalization</li><li>Dropout</li><li>L1/L2 regularization</li><li>Learning rate scheduling</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1502.03167>Batch Normalization paper</a></li><li><a href=https://jmlr.org/papers/v15/srivastava14a.html>Dropout paper</a></li><li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4">Stanford CS224n lectures</a></li></ul><p><strong>Project:</strong></p><ul><li>Build a deep network for sentiment analysis with regularization techniques</li></ul><h2 id=week-5-sequential-data--rnns>Week 5: Sequential Data & RNNs<a hidden class=anchor aria-hidden=true href=#week-5-sequential-data--rnns>#</a></h2><p><strong>Topics:</strong></p><ul><li>RNN architecture</li><li>Backpropagation through time</li><li>Vanishing/exploding gradients</li><li>LSTM cells</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://www.bioinf.jku.at/publications/older/2604.pdf>LSTM paper</a></li><li><a href=https://karpathy.github.io/2015/05/21/rnn-effectiveness/>Andrej Karpathy&rsquo;s blog on RNNs</a></li><li><a href=https://colah.github.io/posts/2015-08-Understanding-LSTMs/>Chris Olah&rsquo;s LSTM blog post</a></li></ul><p><strong>Project:</strong></p><ul><li>Character-level text generation using LSTM</li></ul><h2 id=week-6-introduction-to-attention-mechanisms>Week 6: Introduction to Attention Mechanisms<a hidden class=anchor aria-hidden=true href=#week-6-introduction-to-attention-mechanisms>#</a></h2><p><strong>Topics:</strong></p><ul><li>Encoder-decoder architecture</li><li>Teacher forcing</li><li>Beam search</li><li>Basic attention mechanisms</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1409.3215>Seq2Seq paper</a></li><li><a href=https://arxiv.org/abs/1409.0473>Neural Machine Translation paper</a></li><li><a href=https://lilianweng.github.io/posts/2018-06-24-attention/>Lilian Weng&rsquo;s attention blog post</a></li></ul><p><strong>Project:</strong></p><ul><li>Implement Bahdanau (additive) or Luong (multiplicative) attention.</li><li>Implement a basic sequence-to-sequence model for translating English to French using Bahdanau attention (use small parallel corpora).</li></ul><h2 id=week-7-self-attention-and-multi-head-attention>Week 7: Self-Attention and Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#week-7-self-attention-and-multi-head-attention>#</a></h2><p><strong>Topics:</strong></p><ul><li>Score functions</li><li>Query-Key-Value concept</li><li>Self-attention</li><li>Dot-product attention vs. additive attention</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a></li><li><a href=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/>Jay Alammar&rsquo;s blog on attention</a></li><li><a href=https://jalammar.github.io/illustrated-transformer/>https://jalammar.github.io/illustrated-transformer/</a></li></ul><p><strong>Project:</strong></p><ul><li>Manually compute self-attention for a toy example and build a self-attention layer using PyTorch.</li><li>Extend the implementation to a multi-head attention mechanism and validate its performance on sequence data.</li></ul><h2 id=week-8-multi-head-attention-and-positional-encoding>Week 8: Multi-Head Attention and Positional Encoding<a hidden class=anchor aria-hidden=true href=#week-8-multi-head-attention-and-positional-encoding>#</a></h2><p><strong>Topics:</strong></p><ul><li>Multi-head attention</li><li>Positional encodings (sinusoidal functions)</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a></li><li><a href=https://jalammar.github.io/illustrated-transformer/>https://jalammar.github.io/illustrated-transformer/</a></li><li><a href=https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853>https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853</a></li><li><a href=https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a>https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a</a></li></ul><p><strong>Project:</strong></p><ul><li>Write code for multi-head attention.</li><li>Implement positional encoding and visualize it.</li><li>Implement a custom multi-head attention module and add positional encodings. Use this to classify sequences of text (e.g., positive/negative sentiment).</li></ul><h2 id=week-9-transformers-block-encoder-decoder-structure>Week 9: Transformers Block (Encoder-Decoder Structure)<a hidden class=anchor aria-hidden=true href=#week-9-transformers-block-encoder-decoder-structure>#</a></h2><p><strong>Topics:</strong></p><ul><li>Encoder and decoder architecture</li><li>Residual connections and layer normalization</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://nlp.seas.harvard.edu/annotated-transformer/>https://nlp.seas.harvard.edu/annotated-transformer/</a></li><li><a href=https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face>https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face</a></li></ul><p><strong>Projects:</strong></p><ul><li>Build a simple transformer encoder layer.</li><li>Build a transformer encoder for a language modeling task using PyTorch or TensorFlow.</li><li>Train the encoder on a small text dataset (e.g., Shakespeare sonnets).</li></ul><h2 id=week-10-full-transformer-model>Week 10: Full Transformer Model<a hidden class=anchor aria-hidden=true href=#week-10-full-transformer-model>#</a></h2><p><strong>Topics:</strong></p><ul><li>End-to-end implementation of the original transformer</li><li>Complete transformer architecture</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a></li><li><a href=https://nlp.seas.harvard.edu/annotated-transformer/>https://nlp.seas.harvard.edu/annotated-transformer/</a></li><li><a href=https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face>https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face</a></li></ul><p><strong>Projects:</strong></p><ul><li>Implement a transformer-based sequence classification task.</li><li>Implement a simplified transformer model from scratch and apply it to text summarization or machine translation.</li><li>Use performance metrics (BLEU score for translation, ROUGE score for summarization) to evaluate results.</li></ul><h2 id=week-11-transformer-variants-bert-gpt>Week 11: Transformer Variants (BERT, GPT)<a hidden class=anchor aria-hidden=true href=#week-11-transformer-variants-bert-gpt>#</a></h2><p><strong>Topics:</strong></p><ul><li>BERT (masked language modeling)</li><li>GPT (causal language modeling)</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/1810.04805>https://arxiv.org/abs/1810.04805</a></li><li><a href=https://arxiv.org/abs/2005.14165>https://arxiv.org/abs/2005.14165</a></li><li><a href=https://huggingface.co/learn/nlp-course/en/chapter4/2>https://huggingface.co/learn/nlp-course/en/chapter4/2</a></li></ul><p><strong>Projects:</strong></p><ul><li>Fine-tune a pre-trained BERT or GPT model using HuggingFace.</li><li>Implement a chatbot using a GPT model for conversational responses.</li></ul><h3 id=additional-project-ideas>Additional Project Ideas<a hidden class=anchor aria-hidden=true href=#additional-project-ideas>#</a></h3><p>Once you complete the core projects, reinforce your learning with larger, integrative projects:</p><ol><li><strong>Sentiment Analysis on Movie Reviews</strong>: Use transformers for sentiment classification on the IMDB dataset.</li><li><strong>Named Entity Recognition (NER)</strong>: Implement NER using transformers and fine-tune on the CoNLL-2003 dataset.</li><li><strong>Question Answering System</strong>: Use BERT or RoBERTa to create a question-answering application on a custom dataset.</li></ol><h2 id=week-12-advanced-techniques-and-optimization><strong>Week 12: Advanced Techniques and Optimization</strong><a hidden class=anchor aria-hidden=true href=#week-12-advanced-techniques-and-optimization>#</a></h2><p><strong>Topics:</strong></p><ul><li>Model distillation</li><li>Reducing memory consumption (efficient transformers)</li></ul><p><strong>Resources:</strong></p><ul><li><a href=https://arxiv.org/abs/2009.06732>https://arxiv.org/abs/2009.06732</a></li><li><a href=https://arxiv.org/abs/2001.04451>https://arxiv.org/abs/2001.04451</a></li><li><a href=https://huggingface.co/docs/transformers/en/training>https://huggingface.co/docs/transformers/en/training</a></li></ul><p><strong>Projects:</strong></p><ul><li>Experiment with efficient transformer architectures (e.g., Reformer or Longformer) for a custom dataset with long sequences.</li><li>Apply model distillation to compress a large transformer model into a smaller, faster one for inference.</li></ul><p>Happy learning!</p><hr><p><em>Get the latest articles on AI delivered straight to your inbox. <a href=https://romeepanchal.substack.com/p/from-logistic-regression-to-transformers>Subscribe here!</a></em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://romitheguru.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://romitheguru.github.io/tags/transformers/>Transformers</a></li><li><a href=https://romitheguru.github.io/tags/learning-path/>Learning-Path</a></li></ul><nav class=paginav><a class=prev href=https://romitheguru.github.io/posts/deep_learning/nlp_progress/><span class=title>« Prev</span><br><span>From LSTMs to RLHF — How One Idea Ignites the Next</span>
</a><a class=next href=https://romitheguru.github.io/posts/life/learnings/self_study_experience/><span class=title>Next »</span><br><span>Learning on Your Own: Challenges, Failures, and Successes</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share From Logistic Regression to Transformers: Learning Path on x" href="https://x.com/intent/tweet/?text=From%20Logistic%20Regression%20to%20Transformers%3a%20Learning%20Path&amp;url=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2flearning_path%2f&amp;hashtags=deeplearning%2ctransformers%2clearning-path"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From Logistic Regression to Transformers: Learning Path on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2flearning_path%2f&amp;title=From%20Logistic%20Regression%20to%20Transformers%3a%20Learning%20Path&amp;summary=From%20Logistic%20Regression%20to%20Transformers%3a%20Learning%20Path&amp;source=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2flearning_path%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From Logistic Regression to Transformers: Learning Path on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2flearning_path%2f&title=From%20Logistic%20Regression%20to%20Transformers%3a%20Learning%20Path"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From Logistic Regression to Transformers: Learning Path on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2flearning_path%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share From Logistic Regression to Transformers: Learning Path on whatsapp" href="https://api.whatsapp.com/send?text=From%20Logistic%20Regression%20to%20Transformers%3a%20Learning%20Path%20-%20https%3a%2f%2fromitheguru.github.io%2fposts%2fdeep_learning%2flearning_path%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://romitheguru.github.io/>Romee's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>