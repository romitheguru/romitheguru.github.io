<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Transformers on Romee&#39;s Blog</title>
    <link>https://romitheguru.github.io/tags/transformers/</link>
    <description>Recent content in Transformers on Romee&#39;s Blog</description>
    <generator>Hugo -- 0.146.5</generator>
    <language>en</language>
    <lastBuildDate>Sat, 12 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://romitheguru.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From LSTMs to RLHF — How One Idea Ignites the Next</title>
      <link>https://romitheguru.github.io/posts/deep_learning/nlp_progress/</link>
      <pubDate>Sat, 12 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://romitheguru.github.io/posts/deep_learning/nlp_progress/</guid>
      <description>&lt;h2 id=&#34;innovation-as-a-relay-race&#34;&gt;Innovation as a Relay Race&lt;/h2&gt;
&lt;p&gt;Every landmark in modern NLP began as a modest answer to a concrete limitation. One group publishes a clever fix, another notices the remaining crack, and a third discovers a shortcut the first two never imagined. LSTMs rescued RNNs from vanishing gradients; contextual embeddings made static vectors obsolete; transformers smashed the parallel-computing ceiling; instruction-tuned models closed the human-alignment gap.&lt;/p&gt;
&lt;p&gt;Tracking that relay race is more instructive than memorising any single result: it shows why each breakthrough mattered and how the field’s centre of gravity kept shifting.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Logistic Regression to Transformers: Learning Path</title>
      <link>https://romitheguru.github.io/posts/deep_learning/learning_path/</link>
      <pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://romitheguru.github.io/posts/deep_learning/learning_path/</guid>
      <description>&lt;p&gt;Are you fascinated by deep learning&amp;rsquo;s transformative power but unsure how to navigate the journey from logistic regression to mastering transformer architectures? You’re not alone. Transformers are the backbone of modern AI, power innovations in natural language processing, computer vision, and beyond, but getting there can feel daunting.&lt;/p&gt;
&lt;p&gt;In this blog, I outline a structured, week-by-week learning path that takes you from the foundational concepts of machine learning to building and fine-tuning your transformer models. Whether you&amp;rsquo;re a beginner or looking to deepen your expertise, this roadmap combines key concepts, curated resources, hands-on projects, and practical tips to make your progress achievable and rewarding.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
